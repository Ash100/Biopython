{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNQTA1t8NF6rePwC5Mp0d8L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ash100/Biopython/blob/main/Saving_your_model_for_Later_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save your model for later with Serialization\n",
        "I am **Dr. Ashfaq Ahmad**, and this notebook is created for teaching and research purposes. Refering to the people working in the field of Biology, I have tried my level best to keep it as simple as possible. For Detailed instruction and understandings, please watch a video tutorial on **https://www.youtube.com/@Bioinformaticsinsights**\n",
        "\n",
        "This notebook is based on the book **Deep Learning with Python** by Jason Brownlee."
      ],
      "metadata": {
        "id": "49sa7hsHRC7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that deep learning models can take hours, days and even weeks to train, it is important to know how to save and load them from disk.\n",
        " In this lesson you will discover how you can save\n",
        "your Keras models to file and load them up again to make predictions. After completing this lesson you will know:\n",
        "How to save and load Keras model weights to HDF5 formatted files.\n",
        "How to save and load Keras model structure to JSON files.\n",
        "How to save and load Keras model structure to YAML files.\n",
        "Letâ€™s get started."
      ],
      "metadata": {
        "id": "McePkbi1RWRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Overview\n",
        "Keras separates the concerns of saving your model architecture and saving your model weights. Model weights are saved to **HDF5** format. This is a grid format that is ideal for storing multi-dimensional arrays of numbers. The model structure can be described and saved (and loaded) using two different formats: **JSON** and **YAML**.\n",
        "Each example will also demonstrate saving and loading your model weights to HDF5\n",
        "formatted files. The examples will use the same simple network trained on the Pima Indians onset of diabetes binary classification dataset"
      ],
      "metadata": {
        "id": "sJcn3zbHSUIQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3xkgnnaQ_cW"
      },
      "outputs": [],
      "source": [
        "#Install h5py\n",
        "!pip install h5py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Keras\n",
        "!pip install keras"
      ],
      "metadata": {
        "id": "_XdgXyTtUHkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Serialization to JSON and HDF5\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import model_from_json\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.losses import binary_crossentropy\n",
        "import numpy\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"/content/sample_data/diabetes_1.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
        "\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "# later...\n",
        "\n",
        "# load json and create model\n",
        "json_file = open(\"model.json\", \"r\")\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"model.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqSh_Aw9Tlq0",
        "outputId": "01684dff-44ca-4beb-c44d-527d8c4c58c2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 79.82%\n",
            "Saved model to disk\n",
            "Loaded model from disk\n",
            "accuracy: 79.82%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save your Neural Network to YAML"
      ],
      "metadata": {
        "id": "dCgU4u0aUl1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Serialization to YAML and HDF5\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import model_from_json\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.losses import binary_crossentropy\n",
        "import numpy\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"/content/sample_data/diabetes_1.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
        "\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.yaml\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "# later...\n",
        "\n",
        "# load json and create model\n",
        "json_file = open(\"model.yaml\", \"r\")\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"model.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81NNTjb5U1_S",
        "outputId": "063c0ed0-7baa-45e0-9f5a-0c5472df3619"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 80.60%\n",
            "Saved model to disk\n",
            "Loaded model from disk\n",
            "accuracy: 80.60%\n"
          ]
        }
      ]
    }
  ]
}