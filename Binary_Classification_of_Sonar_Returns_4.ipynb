{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPOH1uSszxlh/DsOJ6b4+vI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ash100/Biopython/blob/main/Binary_Classification_of_Sonar_Returns_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Binary Classification of Sonar Returns\n",
        "I am **Dr. Ashfaq Ahmad**, and this notebook is created for teaching and research purposes. Refering to the people working in the field of Biology, I have tried my level best to keep it as simple as possible. For Detailed instruction and understandings, please watch a video tutorial on **https://www.youtube.com/@Bioinformaticsinsights**\n",
        "\n",
        "This notebook is based on the book **Deep Learning with Python** by Jason Brownlee."
      ],
      "metadata": {
        "id": "pvcsFJXSyg8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial we will discover how to use the Keras library in machine\n",
        "learning project by working through a binary classification project step-by-step.\n",
        "##Aims and Objectives\n",
        "**1.** How to load training data and make it available to Keras.\n",
        "**2.** How to design and train a neural network for tabular data.\n",
        "**3.** How to evaluate the performance of a neural network model in Keras on unseen data.\n",
        "**4.** How to perform data preparation to improve skill when using neural networks.\n",
        "**5.** How to tune the topology and configuration of neural networks in Keras.\n",
        "Let’s get started."
      ],
      "metadata": {
        "id": "AR4h9aFey3RF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##About the Dataset\n",
        "This is the data set used by Gorman and Sejnowski in their study\n",
        "of the classification of sonar signals using a neural network. The\n",
        "task is to train a network to discriminate between sonar signals bounced\n",
        "off a metal cylinder and those bounced off a roughly cylindrical rock.\n",
        "\n",
        "There are 60 input variables that returns at different angles. It is a binary classification problem that requires a model to differentiate rocks from metal cylinders.\n",
        "\n",
        "The output variable is a string **M** for mine and **R** for rock, which will need to be converted to integers 1 and 0. The dataset contains 208 observations. The dataset can be downloaded from Kaggle."
      ],
      "metadata": {
        "id": "AcY7DKnazfzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Neural Network and Performance"
      ],
      "metadata": {
        "id": "koxSs9q00-GO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Keras and Scikit\n",
        "!pip install --upgrade keras\n",
        "!pip install --upgrade scikit_learn"
      ],
      "metadata": {
        "id": "eYNXXBYW1inV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Install as per your need\n",
        "!pip install scikeras[tensorflow-cpu]"
      ],
      "metadata": {
        "id": "-0ZR1E-A1n9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Install as per your need\n",
        "!pip install scikeras[tensorflow]      # gpu compute platform"
      ],
      "metadata": {
        "id": "FMWD7VYf1s_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import pandas\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "nKV_4vK82GDe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)"
      ],
      "metadata": {
        "id": "GdLab4mI2UfE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "dataframe = pandas.read_csv(\"/content/sample_data/sonar.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:60].astype(float)\n",
        "Y = dataset[:,60]"
      ],
      "metadata": {
        "id": "plo7ugjt204u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output variable is string values **(R or M)**. We must convert them into integer values 0 and 1. We can do this using the LabelEncoder class from scikit-learn. This class will model the encoding required using the entire dataset via the *fit()* function, then apply the encoding to create a\n",
        "new output variable using the *transform()* function."
      ],
      "metadata": {
        "id": "urVz9NwF3K2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)"
      ],
      "metadata": {
        "id": "rPQ1_aH13cVv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to create our neural network model using Keras. We are going to use scikit-learn to evaluate the model using stratified k-fold cross validation. This is a resampling technique that will provide an estimate of the performance of the model."
      ],
      "metadata": {
        "id": "egLLsXV83vN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use Keras models with scikit-learn, we must use the KerasClassifier wrapper. This class takes a function that\n",
        "creates and returns our neural network model. It also takes arguments that it will pass along to the call to *fit()* such as the number of epochs and the batch size.\n",
        "\n",
        "Let’s start off by defining the function that creates our baseline model. Our model will have a single fully connected hidden layer with the same number of neurons as input variables. This is a good default starting point\n",
        "when creating neural networks on a new problem.\n",
        "\n",
        "The weights are initialized using a small Gaussian random number. The Rectifier activation function is used. The output layer contains a single neuron in order to make predictions. It uses the sigmoid activation function in order to produce a probability output in the range of\n",
        "0 to 1 that can easily and automatically be converted to crisp class values.\n",
        "\n",
        "Finally, we are using the logarithmic loss function (binary crossentropy) during training, the preferred loss\n",
        "function for binary classification problems. The model also uses the Adam optimization algorithm for gradient descent and accuracy metrics will be collected when the model is trained."
      ],
      "metadata": {
        "id": "O22VO2_64L0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.activations import relu, sigmoid\n",
        "\n",
        "# Define baseline model\n",
        "def create_baseline():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
        "    # Compile model\n",
        "    model.compile(loss=binary_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "4hs8BY4i4718"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initial Model Evaluation\n",
        "Now it is time to evaluate this model using stratified cross validation in the scikit-learn framework. We pass the number of training epochs to the KerasClassifier, again using\n",
        "reasonable default values. Verbose output is also turned off given that the model will be created 10 times for the 10-fold cross validation being performed."
      ],
      "metadata": {
        "id": "I9H-Ix9N5V4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model with standardized dataset\n",
        "estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(estimator, X, encoded_Y, cv=kfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "metadata": {
        "id": "wiLmsg0T5lKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running this code produces the Above output showing the mean and standard deviation of the estimated accuracy of the model on unseen data"
      ],
      "metadata": {
        "id": "1iCk2VK-6pHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Improve Model Performance with Data Preparation\n",
        "It is a good practice to prepare your data before modeling. Neural network models are especially suitable to having consistent input values, both in scale and distribution. An effective data preparation scheme for tabular data when building neural network models is standardization.\n",
        "\n",
        "This is where the data is rescaled such that the mean value for each attribute is 0 and the standard deviation is 1. This preserves Gaussian and Gaussian-like distributions whilst normalizing the central tendencies for each attribute.\n",
        "\n",
        "We can use scikit-learn to perform the standardization of our Sonar dataset using the StandardScaler class. Rather than performing the standardization on the entire dataset, it is good practice to train the standardization procedure on the training data within the pass of a cross validation run and to use the trained standardization example to prepare the unseen test fold.\n",
        "\n",
        "This makes standardization a step in model preparation in the cross validation process and it prevents the algorithm having knowledge of unseen data during evaluation, knowledge\n",
        "that might be passed from the data preparation scheme like a crisper distribution."
      ],
      "metadata": {
        "id": "IxR4M55v60Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Classification with Sonar Dataset: Standardized\n",
        "import numpy\n",
        "import pandas\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.activations import relu, sigmoid\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load dataset\n",
        "dataframe = pandas.read_csv(\"/content/sample_data/sonar.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:60].astype(float)\n",
        "Y = dataset[:,60]\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "\n",
        "# define baseline model\n",
        "def create_baseline():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# evaluate baseline model with standardized dataset\n",
        "numpy.random.seed(seed)\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100,\n",
        "                                           batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "metadata": {
        "id": "HOH5Z6Vv8cec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Time to Layer Tunning in the Model"
      ],
      "metadata": {
        "id": "-qAchsci8-Gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many things to tune on a neural network, such as the weight initialization, activation functions, optimization procedure and so on. One aspect that may have an outsized effect is the structure of the network itself called the network topology. In this section we take a look at two experiments on the structure of the network: **making it smaller and making it larger**. These are\n",
        "good experiments to perform when tuning a neural network on your problem"
      ],
      "metadata": {
        "id": "xBohjwcn9cLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test / Evaluate a Smaller Network\n",
        "I suspect that there is a lot of redundancy in the input variables for this problem. The data describes the same signal from different angles. We can force a type of feature extraction by the network by restricting the representational space in the first hidden layer.\n",
        "\n",
        "In this experiment we take our baseline model with 60 neurons in the hidden layer and reduce it by half to 30.\n",
        "\n",
        "This will put pressure on the network during training to pick out the most important structure in the input data to model. We will involve the standardize step in the previous experiment with data preparation and try to take advantage of the small lift in performance.\n"
      ],
      "metadata": {
        "id": "UYkJvJDO9qLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Binary classification with Sonar Dataset: Standardize Smaller\n",
        "import numpy\n",
        "import pandas\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.activations import relu, sigmoid\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load dataset\n",
        "dataframe = pandas.read_csv(\"/content/sample_data/sonar.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:60].astype(float)\n",
        "Y = dataset[:,60]\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "\n",
        "# define smaller model\n",
        "def create_smaller():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(30, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# evaluate smaller model with standardized dataset\n",
        "numpy.random.seed(seed)\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=100,\n",
        "                                           batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Smaller: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "metadata": {
        "id": "2ONz4DI1-i6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluate Larger Network\n",
        "A neural network topology with more layers offers more opportunity for the network to extract key features and recombine them in useful nonlinear ways. We can evaluate whether adding more layers to the network improves the performance easily by making another small tweak to\n",
        "the function used to create our model.\n",
        "\n",
        "Here, we add one new layer (one line) to the network\n",
        "that introduces another hidden layer with 30 neurons after the first hidden layer. Our network now has the topology:\n",
        "60 inputs -> [60 -> 30] -> 1 output\n",
        "\n",
        "The idea here is that the network is given the opportunity to model all input variables before being bottlenecked and forced to halve the representational capacity, much like we did in the experiment above with the smaller network. Instead of squeezing the representation of the\n",
        "inputs themselves, we have an additional hidden layer to aid in the process."
      ],
      "metadata": {
        "id": "8fpvkjtn_nZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Binnary Classification with Sonar Dataset: Standardized Larger\n",
        "import numpy\n",
        "import pandas\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.activations import relu, sigmoid\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load dataset\n",
        "dataframe = pandas.read_csv(\"/content/sample_data/sonar.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:60].astype(float)\n",
        "Y = dataset[:,60]\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "\n",
        "# define larger model\n",
        "def create_larger():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dense(30, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# evaluate larger model with standardized dataset\n",
        "numpy.random.seed(seed)\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_larger, epochs=100,\n",
        "                                           batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "metadata": {
        "id": "YbZLn4r5AHpg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}