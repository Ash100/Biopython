{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNk/sE3uHDVhcRwIvt+OC/U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ash100/Biopython/blob/main/Introduction_of_checkpoints_during_model_Training_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Introducing Check-points and keep the best model during training\n",
        "I am **Dr. Ashfaq Ahmad**, and this notebook is created for teaching and research purposes. Refering to the people working in the field of Biology, I have tried my level best to keep it as simple as possible. For Detailed instruction and understandings, please watch a video tutorial on **https://www.youtube.com/@Bioinformaticsinsights**\n",
        "\n",
        "This notebook is based on the book **Deep Learning with Python** by Jason Brownlee."
      ],
      "metadata": {
        "id": "0ihhayBiANuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep learning models can take hours, days or even weeks to train and if a training run is stopped unexpectedly, you can lose a lot of work. In this lesson you will discover how you can create a checkpoint in your deep learning models during training in Python using the Keras library."
      ],
      "metadata": {
        "id": "o9b4gUtfF1rf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Aims and Objectives\n",
        "How to introduce checkpoint for each improvement to a model during training.\n",
        "\n",
        "How to checkpoint the very best model observed during training.\n",
        "\n",
        "Let’s get started."
      ],
      "metadata": {
        "id": "7g1nuAppF_QO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Keras library provides a checkpointing capability by a callback API. The ModelCheckpoint callback class allows you to define where to checkpoint the model weights, how the file should be named and under what circumstances to make a checkpoint of the model. The API allows you to specify which metric to monitor, such as loss or accuracy on the training or validation dataset. You can specify whether to look for an improvement in maximizing or minimizing the score. Finally, the filename that you use to store the weights can include variables like the epoch number or metric. The ModelCheckpoint instance can then be passed to the training\n",
        "process when calling the fit() function on the model."
      ],
      "metadata": {
        "id": "l2_OVrKzKKaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Rule of thumb\n",
        "A good use of checkpointing is to output the model weights each time an improvement is observed during training. The example below creates a small neural network for the Pima\n",
        "Indians onset of diabetes binary classification problem. The example uses 33% of the data for validation.\n",
        "Checkpointing is setup to save the network weights only when there is an improvement in classification accuracy on the validation dataset (monitor=’val acc’ and mode=’max’). The\n",
        "weights are stored in a file that includes the score in the filename weights-improvement-val acc=.2f.hdf5."
      ],
      "metadata": {
        "id": "rMJojhtCKox8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIFFRBGUAM_v"
      },
      "outputs": [],
      "source": [
        "#Install Keras\n",
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpoint the weights when validation accuracy improves\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"/content/sample_data/diabetes_1.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)"
      ],
      "metadata": {
        "id": "EGjddq5qLnW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the output file you can read the accuracy statements, and is simple to understand."
      ],
      "metadata": {
        "id": "GFC1c5YpMUpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Check point for the best neural network model\n",
        "A simpler checkpoint strategy is to save the model weights to the same file, **if and only if the validation accuracy improves**. This can be done easily using the same code from above and changing the output filename to be fixed (not include score or epoch information). In this case,\n",
        "model weights are written to the file **weights.best.hdf5** only if the classification accuracy of the model on the validation dataset improves over the best seen so far."
      ],
      "metadata": {
        "id": "f_E_Rt3SMrQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpoint the weights for best model on validation accuracy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"/content/sample_data/diabetes_1.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# checkpoint\n",
        "filepath=\"weights.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)"
      ],
      "metadata": {
        "id": "02PFB8akM959"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##How to load the model saved with checkpointing\n",
        "We assume that we need to load the **weights.best.hdf5** model."
      ],
      "metadata": {
        "id": "4s20zgOeN0rK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#How to load and use weights from checkpoints\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\n",
        "# load weights\n",
        "model.load_weights(\"weights.best.hdf5\")\n",
        "\n",
        "# Compile model (required to make predictions)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(\"Created model and loaded weights from file\")\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"/content/sample_data/diabetes_1.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# estimate accuracy on whole dataset using loaded weights\n",
        "scores = model.evaluate(X, Y, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "metadata": {
        "id": "cpTCXtgDOmzY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}